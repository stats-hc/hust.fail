---
title: '[Advanced Statistics Notes]U-statistics'
author: Chen Huang
date: '2021-02-28'
slug: advanced-statistics-notes-u-statistics
categories:
  - note
tags: ["U-statistics", "Nonparametric statistics", "Mathematical Statistics"]
output:
  blogdown::html_page:
    number_sections: true
---



<div id="u-statistics" class="section level1">
<h1><span class="header-section-number">1</span> U-Statistics</h1>
<p>Let <span class="math inline">\(X_1, \cdots,X_n\overset{iid}{\sim}P\in\mathcal{P}\)</span>.In a large class of problems, parameters to be estimated are of the form
<span class="math display">\[\theta = \mathbf{E}h(X_1, \cdots, X_m)\]</span>
with a fixed positive integer <span class="math inline">\(m\)</span>, where <span class="math inline">\(h:\mathcal{X}\rightarrow \mathbb{R}\)</span> is symmetric in its arguments and satisfies <span class="math inline">\(\mathbf{E}|h(X_{i_1}, \cdots, X_{i_m}|&lt;\infty\)</span> for any <span class="math inline">\(P\in\mathcal{P}\)</span>.<br />
A U-statistics with kernel <span class="math inline">\(h\)</span> of order <span class="math inline">\(m\)</span> is
<span class="math display">\[
U_n = \left(\begin{array}{c}n\\m\end{array}\right)^{-1}\sum_{c} h(X_{i_1},\cdots, X_{i_m}) 
\]</span>
where <span class="math inline">\(\sum_{c}\)</span> denotes a summation over all m-subsets of <span class="math inline">\(\{1,\cdots,n\}\)</span>. It is easy to see that it is an unbiased estimator of <span class="math inline">\(\theta\)</span>.</p>
</div>
<div id="examples" class="section level1">
<h1><span class="header-section-number">2</span> Examples</h1>
<ol style="list-style-type: decimal">
<li>Let <span class="math inline">\(m=1\)</span> and <span class="math inline">\(h(x) = x^k, k=1,2,\cdots\)</span>. <span class="math inline">\(U_n\)</span> becomes the <em>sample moments</em>:
<span class="math display">\[U_n = \frac{1}{n}\sum_{i=1}^nX_i^k\]</span></li>
<li>Let <span class="math inline">\(m=1\)</span> and <span class="math inline">\(h(x) = \mathbf{1}(x\leq t)\)</span>, we obtain the <em>empirical c.d.f</em>
<span class="math display">\[
U_n = \frac{1}{n}\sum_{i=1}^n\mathbf{1}(X_i\leq t)
\]</span>
is an unbiased estimator of <span class="math inline">\(F(t)\)</span>.</li>
<li>Consider the estimation of <span class="math inline">\(\theta = \mu^m\)</span>, where <span class="math inline">\(\mu = \mathbf{E}X_1\)</span> and <span class="math inline">\(m&gt;1\)</span> is an integer.Let
<span class="math inline">\(h(x_1,\cdots,x_m) = x_1\cdots x_n\)</span>, we obtain an unbiased estimator of <span class="math inline">\(\mu^m\)</span>:
<span class="math display">\[U_{n}=\left(\begin{array}{c}
n \\
m
\end{array}\right)^{-1} \sum_{c} X_{i_{1}} \cdots X_{i_{m}}\]</span></li>
<li>Consider the estimation of <span class="math inline">\(\theta = \sigma^2=\mathbf{Var}(X_1)\)</span>.Let <span class="math inline">\(h(x_1, x_2) = (x_1-x_2)^2/2\)</span>, we obtain the following U-statistics
<span class="math display">\[
U_{n}=\frac{2}{n(n-1)} \sum_{1 \leq i&lt;j \leq n} \frac{\left(X_{i}-X_{j}\right)^{2}}{2}=\frac{1}{n-1}\left(\sum_{i=1}^{n} X_{i}^{2}-n \bar{X}^{2}\right)=S^{2}
\]</span>
which is the <em>sample variance</em>.</li>
<li>Let <span class="math inline">\(\theta = P(X_1+X_2)\leq t\)</span>, using the kernel <span class="math inline">\(h(x_1,x_2)=\mathbf{1}(x_1+x_2)\)</span>, we obtain the following U-statistics
<span class="math display">\[U_{n}=\frac{2}{n(n-1)} \sum_{1 \leq i&lt;j \leq n} I_{(-\infty, 0]}\left(X_{i}+X_{j}\right)\]</span>
which is the <em>one-sample Wilcoxon statistics</em>.</li>
</ol>
</div>
<div id="variance-of-u-statistics" class="section level1">
<h1><span class="header-section-number">3</span> Variance of U-statistics</h1>
<p>If <span class="math inline">\(E\left[h\left(X_{1}, \ldots, X_{m}\right)\right]^{2}&lt;\infty\)</span>, then the variance of <span class="math inline">\(U_n\)</span> has an explicit form.</p>
</div>
